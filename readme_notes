General Idea of scraper:

execute once on all available historic data to have a set for validation

execute each night at 4 AM CET to make sure no new articles are released during run

scraper:

  for each website
    get links to press releases either from RSS or news-subsite
      safe link, date, headline in file "YYYY_MM_DD_links"
      safe error messages in a seperate file "YYYY_MM_DD_links_errors"
      discard dublicate entries or entries that are already >=3 x in data base
    open each individual of these website
      safe complete website as string in one file "YYYY_MM_DD_websites"
      safe error messages in a seperate file "YYYY_MM_DD_websites_errors"
    open each website-string
      safe headline, date, text body, in one file "YYYY_MM_DD_data"
      safe error messages in a seperate file "YYYY_MM_DD_data_errors"
    combine all error files in one "YYYY_MM_DD_errors" file
    for each entry in the "YYYY_MM_DD_data" file
      check whether keyword e.g. "streik", "warnstreik" is in headline or text body
      extract all city names and match geo locations
      extract all company names
      extract all numbers
      extract all temporal information (e.g. "X hours", "X days")
    send e-mail with the following information
      new articles: X
      new articles that seem to be about a strike: X
      number of errors in scraper_links: X
      number of errors in scraper_websites: X
      elapsed time scraper_links: X
      elapsed time scraper_websites: X
      share of external links: X
        -> safe all of this information in a meta-file "YYYY_MM_DD_meta"
    update strike count on website with geo locations
  -> files created
    1. YYYY_MM_DD_links
    2. YYYY_MM_DD_websites
    3. YYYY_MM_DD_data
    4. YYYY_MM_DD_errors
    5. YYYY_MM_DD_meta

Back of the envelope calculation for storage space:
  50,000 historic press releases (depth 200 sites historic) require 8 MB
  one full website requires 100 KB
    5 GB for 50,000 historic press releases at depth 200 sites -> feasible
    given ca 400 articles / month, assume 30 articles / day, 3 MB for daily scrape
      ca 1 GB / year -> feasible

current run time
  historic press releases 200 depth 2023-03-11: 48726 entries, 266 min




